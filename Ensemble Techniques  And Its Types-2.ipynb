{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232046e0-a93b-4ea7-96a3-594f37203c08",
   "metadata": {},
   "source": [
    "# Q1. How does bagging reduce overfitting in decision trees?\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique that aims to reduce overfitting and improve the generalization performance of machine learning models, particularly decision trees. Here's how bagging helps reduce overfitting in decision trees:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Bagging involves creating multiple bootstrap samples from the original training dataset. Each bootstrap sample is obtained by randomly sampling with replacement from the original dataset. As a result, some instances may appear multiple times in a given bootstrap sample, while others may be omitted.\n",
    "\n",
    "2. **Training Multiple Trees:**\n",
    "   - A decision tree is trained on each of the bootstrap samples independently. Each tree explores a slightly different subset of the original dataset due to the random sampling with replacement. As a result, the individual trees are likely to capture different patterns and noise in the data.\n",
    "\n",
    "3. **Variability Among Trees:**\n",
    "   - Since each tree is trained on a slightly different subset of the data, the individual trees in the ensemble will have different structures and make different predictions. This variability among the trees is crucial for reducing overfitting because it prevents the ensemble from relying too heavily on any particular idiosyncrasy or noise in the training data.\n",
    "\n",
    "4. **Averaging Predictions:**\n",
    "   - In the case of bagging with decision trees, the final prediction is often made by averaging the predictions of all individual trees (for regression tasks) or by taking a majority vote (for classification tasks). The averaging or voting process helps smooth out the impact of individual trees making overly complex or overfit predictions.\n",
    "\n",
    "5. **Reduction of Variance:**\n",
    "   - The main source of overfitting in decision trees is their tendency to fit the training data too closely, capturing noise and outliers. By training multiple trees on different subsets and combining their predictions, bagging helps reduce the variance of the overall model. Variance reduction is particularly beneficial when dealing with complex models prone to overfitting.\n",
    "\n",
    "6. **Improved Generalization:**\n",
    "   - The ensemble of bagged trees tends to generalize better to unseen data because it has learned to make predictions based on the common patterns in the data rather than fitting the noise present in individual instances. This is especially important for improving performance on the test data and avoiding overfitting to the training data.\n",
    "\n",
    "In summary, bagging reduces overfitting in decision trees by promoting model diversity through bootstrap sampling and combining predictions in a way that mitigates the impact of individual trees' idiosyncrasies. The resulting ensemble is more robust and better able to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b9364-4990-488e-9754-5de2438d446e",
   "metadata": {},
   "source": [
    "# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple base learners on different subsets of the data and combining their predictions. The choice of base learners can impact the performance and characteristics of the bagged ensemble. Here are some advantages and disadvantages of using different types of base learners in bagging:\n",
    "\n",
    "### Decision Trees:\n",
    "\n",
    "**Advantages:**\n",
    "- **Flexibility:** Decision trees are versatile and can capture complex relationships in the data.\n",
    "- **Handling Non-linearity:** Effective at capturing non-linear relationships, making them suitable for a wide range of problems.\n",
    "- **Interpretability:** Decision trees are relatively interpretable, making it easier to understand and explain the model.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Vulnerability to Overfitting:** Decision trees can be prone to overfitting, especially if they are deep and capture noise in the data.\n",
    "- **High Variance:** Individual decision trees can have high variance, leading to variability in predictions.\n",
    "\n",
    "### Random Forests (Ensemble of Decision Trees):\n",
    "\n",
    "**Advantages:**\n",
    "- **Reduced Overfitting:** Random Forests address the overfitting problem by building multiple trees on different subsets of data and averaging predictions.\n",
    "- **Improved Generalization:** The ensemble nature of Random Forests often results in improved generalization to unseen data.\n",
    "- **Feature Importance:** Random Forests provide a measure of feature importance, helping in feature selection.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Complexity:** Training multiple decision trees can be computationally expensive, especially for large datasets and deep trees.\n",
    "- **Less Interpretability:** While Random Forests offer some interpretability, the combination of multiple trees makes it more challenging to interpret compared to a single decision tree.\n",
    "\n",
    "### Bagged SVM (Support Vector Machines):\n",
    "\n",
    "**Advantages:**\n",
    "- **Non-linearity:** SVMs with non-linear kernels can capture complex decision boundaries.\n",
    "- **Effective in High-Dimensional Spaces:** SVMs can perform well in high-dimensional feature spaces.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Intensity:** SVMs, especially with non-linear kernels, can be computationally expensive, and bagging exacerbates this.\n",
    "- **Sensitivity to Hyperparameters:** SVMs have hyperparameters (e.g., kernel parameters, regularization parameter) that need careful tuning.\n",
    "\n",
    "### Bagged K-Nearest Neighbors (KNN):\n",
    "\n",
    "**Advantages:**\n",
    "- **Robust to Outliers:** KNN is generally robust to outliers, and bagging can further enhance this robustness.\n",
    "- **No Assumption of Linearity:** KNN makes no assumption about the distribution of data and can capture complex relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Complexity:** KNN has a higher computational cost, especially for large datasets or high-dimensional feature spaces.\n",
    "- **Local Sensitivity:** KNN can be sensitive to local patterns, and bagging may not fully address this sensitivity.\n",
    "\n",
    "### Bagged Linear Regression:\n",
    "\n",
    "**Advantages:**\n",
    "- **Interpretability:** Linear regression is highly interpretable and provides clear insights into the relationship between predictors and the target.\n",
    "- **Efficiency:** Training linear regression models is computationally efficient.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Limited Flexibility:** Linear regression assumes a linear relationship between predictors and the target, which may not capture complex patterns.\n",
    "- **Vulnerability to Assumption Violations:** Linear regression relies on assumptions such as linearity and homoscedasticity, and violations of these assumptions can impact performance.\n",
    "\n",
    "### Bagged Neural Networks:\n",
    "\n",
    "**Advantages:**\n",
    "- **Non-linearity:** Neural networks can capture non-linear relationships and complex patterns.\n",
    "- **Representation Learning:** Neural networks can automatically learn hierarchical representations from the data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Complexity:** Training neural networks can be computationally intensive, especially for deep architectures.\n",
    "- **Sensitivity to Hyperparameters:** Neural networks have many hyperparameters that require careful tuning, and bagging may not fully address this sensitivity.\n",
    "\n",
    "In summary, the choice of base learners in bagging depends on the specific characteristics of the data, the problem at hand, and computational considerations. The advantages and disadvantages listed above highlight some general trends, but empirical evaluation on the specific task is often necessary to determine the most suitable base learner for bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf184b05-77bd-4fda-a047-9e5ee4d3e826",
   "metadata": {},
   "source": [
    "# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "The choice of base learner in bagging can significantly impact the bias-variance tradeoff. Different base learners have distinct characteristics in terms of bias and variance, and bagging is designed to leverage these characteristics to reduce overall variance. Here's how the choice of base learner affects the bias-variance tradeoff in bagging:\n",
    "\n",
    "### Decision Trees:\n",
    "\n",
    "**High Variance, Low Bias:**\n",
    "- **Individual Trees:** Decision trees, especially deep ones, tend to have high variance and low bias. They can fit the training data closely, capturing noise and outliers.\n",
    "- **Bagging Impact:** Bagging helps by averaging the predictions of multiple trees, thereby reducing the overall variance. It works particularly well when the individual trees are diverse.\n",
    "\n",
    "### Random Forests (Ensemble of Decision Trees):\n",
    "\n",
    "**Moderate Bias, Low Variance:**\n",
    "- **Individual Trees:** Random Forests reduce overfitting by training multiple decision trees on different subsets of data. Each tree contributes to capturing different patterns.\n",
    "- **Bagging Impact:** The combination of diverse trees in the ensemble tends to reduce variance while maintaining a moderate level of bias. This results in improved generalization to new data.\n",
    "\n",
    "### Bagged SVM (Support Vector Machines):\n",
    "\n",
    "**Moderate Bias, Low Variance:**\n",
    "- **Individual SVMs:** SVMs can have moderate bias and low variance, especially with appropriate kernel functions.\n",
    "- **Bagging Impact:** Bagging multiple SVMs helps in reducing variance further and can lead to an overall model with lower variance.\n",
    "\n",
    "### Bagged K-Nearest Neighbors (KNN):\n",
    "\n",
    "**Low Bias, High Variance:**\n",
    "- **Individual KNN:** KNN tends to have low bias as it makes few assumptions about the data. However, it can have high variance, especially when the value of \\(k\\) is small.\n",
    "- **Bagging Impact:** Bagging KNN can be effective in reducing variance, making the overall model more robust, especially when the neighborhood size is small.\n",
    "\n",
    "### Bagged Linear Regression:\n",
    "\n",
    "**Low Bias, Moderate Variance:**\n",
    "- **Individual Linear Regression:** Linear regression models often have low bias, especially when the relationship between predictors and the target is approximately linear.\n",
    "- **Bagging Impact:** Bagging linear regression models can further reduce variance, resulting in a model with lower overall bias and moderate variance.\n",
    "\n",
    "### Bagged Neural Networks:\n",
    "\n",
    "**Low Bias, High Variance:**\n",
    "- **Individual Neural Networks:** Neural networks can have low bias due to their ability to capture complex patterns. However, they often come with high variance, especially for deep architectures or limited data.\n",
    "- **Bagging Impact:** Bagging neural networks can be beneficial in reducing overall variance, leading to a more robust model.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Bias Reduction:** Bagging tends to reduce bias when applied to base learners with moderate to high bias, as the ensemble can capture diverse patterns present in the data.\n",
    "  \n",
    "- **Variance Reduction:** Bagging is particularly effective in reducing variance when applied to base learners with high variance. It achieves this by averaging or combining predictions from diverse models.\n",
    "\n",
    "- **Balance:** The goal is to strike a balance between bias and variance. While some bias reduction is desirable, excessively reducing bias may result in a loss of important patterns in the data.\n",
    "\n",
    "In conclusion, the choice of base learner affects the bias-variance tradeoff in bagging by influencing the characteristics of the individual models in the ensemble. The combination of diverse models through bagging aims to achieve a balance that results in improved generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4de97a-c7c4-479c-b54e-7f619d6bae92",
   "metadata": {},
   "source": [
    "# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "Yes, bagging (Bootstrap Aggregating) can be used for both classification and regression tasks. The general idea behind bagging remains the same for both types of tasks — it involves training multiple base learners on different subsets of the data and combining their predictions. However, there are some differences in how bagging is applied to classification and regression problems:\n",
    "\n",
    "### Bagging for Classification:\n",
    "\n",
    "1. **Base Learners:**\n",
    "   - In the context of classification, the base learners are typically classifiers such as decision trees, support vector machines, k-nearest neighbors, or even more complex models like neural networks.\n",
    "\n",
    "2. **Prediction Aggregation:**\n",
    "   - The predictions of individual base learners are combined using methods such as majority voting (for binary or multiclass classification) or averaging probabilities (for probabilistic classifiers).\n",
    "\n",
    "3. **Ensemble Decision:**\n",
    "   - The final ensemble decision is often determined by the class with the highest vote or the class with the highest average probability.\n",
    "\n",
    "4. **Example: Random Forests:**\n",
    "   - Random Forests are a popular ensemble method for classification that involves bagging decision trees. Each tree is trained on a different bootstrap sample, and the final prediction is based on a majority vote among the trees.\n",
    "\n",
    "### Bagging for Regression:\n",
    "\n",
    "1. **Base Learners:**\n",
    "   - In regression tasks, the base learners are typically regression models such as linear regression, decision trees, support vector machines, or other algorithms suitable for regression.\n",
    "\n",
    "2. **Prediction Aggregation:**\n",
    "   - The predictions of individual base learners are combined by averaging (for mean prediction) or using other aggregation methods suitable for regression.\n",
    "\n",
    "3. **Ensemble Prediction:**\n",
    "   - The final ensemble prediction is often the average of the predictions made by individual base learners.\n",
    "\n",
    "4. **Example: Bagged Decision Trees for Regression:**\n",
    "   - Bagging can be applied to decision trees for regression tasks. Each tree is trained on a different bootstrap sample, and the final prediction is the average of the predictions made by individual trees.\n",
    "\n",
    "### Common Aspects:\n",
    "\n",
    "- **Diversity of Base Learners:**\n",
    "   - The effectiveness of bagging relies on the diversity of the base learners. By training on different subsets of data, the base learners capture different aspects of the underlying patterns in the data.\n",
    "\n",
    "- **Bootstrap Sampling:**\n",
    "   - The process of creating multiple bootstrap samples (random samples with replacement) is a common aspect of bagging for both classification and regression.\n",
    "\n",
    "- **Reduction of Overfitting:**\n",
    "   - One of the primary benefits of bagging is its ability to reduce overfitting by combining predictions from multiple models.\n",
    "\n",
    "- **Parallelization:**\n",
    "   - Bagging is well-suited for parallelization, as each base learner can be trained independently. This makes it computationally efficient and scalable.\n",
    "\n",
    "### Differences:\n",
    "\n",
    "- **Prediction Aggregation Method:**\n",
    "   - The way predictions are aggregated differs between classification and regression. Classification tasks typically involve voting or averaging probabilities, while regression tasks involve simple averaging.\n",
    "\n",
    "- **Decision Rule:**\n",
    "   - In classification, the final decision is often based on a decision rule (e.g., majority vote), while in regression, the final prediction is a continuous value.\n",
    "\n",
    "- **Evaluation Metrics:**\n",
    "   - The evaluation metrics used for assessing the performance of the bagged ensemble may differ between classification (e.g., accuracy, precision, recall) and regression (e.g., mean squared error, R-squared).\n",
    "\n",
    "In summary, while the core concept of bagging remains consistent across classification and regression tasks, there are differences in how predictions are aggregated and how the final decision or prediction is determined. The choice of base learners and the specific method for combining predictions depend on the nature of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3dd6e-7537-415b-b33b-f17410be1e89",
   "metadata": {},
   "source": [
    "# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "The ensemble size in bagging refers to the number of base models (learners or classifiers) included in the ensemble. The choice of ensemble size plays a crucial role in the effectiveness of bagging. Here are some considerations regarding the role of ensemble size in bagging:\n",
    "\n",
    "### Role of Ensemble Size:\n",
    "\n",
    "1. **Bias and Variance:**\n",
    "   - As the ensemble size increases, the bias of the model typically remains unchanged or may decrease slightly. However, the variance tends to decrease significantly with a larger ensemble size. This is a key characteristic of bagging — it is particularly effective in reducing variance.\n",
    "\n",
    "2. **Decrease in Overfitting:**\n",
    "   - Larger ensemble sizes are generally associated with a greater reduction in overfitting. When individual models are diverse, combining their predictions helps to smooth out idiosyncrasies and noise present in any single model.\n",
    "\n",
    "3. **Diminishing Returns:**\n",
    "   - There is a point of diminishing returns with respect to ensemble size. After a certain point, adding more models to the ensemble may lead to marginal improvements or may not improve performance at all. In some cases, it could even lead to overfitting the training data.\n",
    "\n",
    "4. **Computational Cost:**\n",
    "   - The computational cost of training and making predictions with the ensemble increases with the ensemble size. Larger ensembles require more memory and processing power. It's essential to strike a balance between model performance and computational efficiency.\n",
    "\n",
    "### How Many Models Should Be Included?\n",
    "\n",
    "1. **Rule of Thumb:**\n",
    "   - A common rule of thumb is to include a sufficiently large number of models to achieve a significant reduction in variance but avoid excessive computational cost. A typical starting point might be in the range of 50 to 500 base models.\n",
    "\n",
    "2. **Empirical Testing:**\n",
    "   - The optimal ensemble size is often determined through empirical testing. Cross-validation or a separate validation set can be used to evaluate the performance of the ensemble for different ensemble sizes.\n",
    "\n",
    "3. **Task and Data Specific:**\n",
    "   - The optimal ensemble size may vary based on the complexity of the task, the characteristics of the data, and the base learners used. Some tasks may benefit from larger ensembles, while others may achieve good performance with a smaller number of models.\n",
    "\n",
    "4. **Monitoring Performance:**\n",
    "   - It's advisable to monitor the performance of the ensemble on a validation set or through cross-validation as the ensemble size changes. This helps identify the point where further increases in ensemble size do not yield significant improvements.\n",
    "\n",
    "5. **Consideration of Resources:**\n",
    "   - Practical considerations, such as available computational resources and time constraints, also play a role in determining the ensemble size. It's important to find a balance that achieves good performance without exceeding resource limits.\n",
    "\n",
    "In summary, the role of ensemble size in bagging is to control the tradeoff between bias and variance. While larger ensembles generally lead to reduced variance and improved generalization, the optimal ensemble size should be determined empirically based on the specific characteristics of the task and data, considering both performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c495f7-61da-4649-b69c-f28e49794154",
   "metadata": {},
   "source": [
    "# Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "Certainly! One real-world application of bagging in machine learning is in the field of remote sensing for land cover classification using satellite imagery. Land cover classification involves categorizing different types of land surfaces, such as forests, urban areas, agricultural fields, and water bodies, based on satellite imagery.\n",
    "\n",
    "### Real-World Application: Land Cover Classification\n",
    "\n",
    "#### Problem Statement:\n",
    "The goal is to develop a machine learning model that accurately classifies land cover types from satellite images. This task is important for various applications, including urban planning, environmental monitoring, and natural resource management.\n",
    "\n",
    "#### Challenges:\n",
    "- Satellite imagery can be affected by factors such as cloud cover, shadows, and seasonal changes, leading to variability in the appearance of land cover types.\n",
    "- The high-dimensional nature of satellite data, with multiple spectral bands and pixels, poses challenges for traditional classifiers.\n",
    "\n",
    "#### Bagging Approach:\n",
    "\n",
    "1. **Base Learners:**\n",
    "   - Decision trees are commonly used as base learners in bagging for land cover classification. Each decision tree is trained on a different subset of the satellite data.\n",
    "\n",
    "2. **Bootstrapped Samples:**\n",
    "   - Multiple bootstrapped samples (random samples with replacement) are created from the original dataset. Each decision tree is trained on a different bootstrapped sample, introducing diversity among the base learners.\n",
    "\n",
    "3. **Ensemble Prediction:**\n",
    "   - The final prediction is made by aggregating the predictions of all individual decision trees. For classification tasks, this often involves a majority vote among the trees.\n",
    "\n",
    "4. **Reducing Overfitting:**\n",
    "   - Bagging helps to reduce overfitting by combining predictions from multiple decision trees. Each tree focuses on capturing different patterns in the data, and the ensemble generalizes well to unseen satellite images.\n",
    "\n",
    "#### Benefits:\n",
    "\n",
    "1. **Robustness to Variability:**\n",
    "   - Bagging improves the robustness of the land cover classification model to variations in satellite imagery caused by factors like cloud cover or seasonal changes.\n",
    "\n",
    "2. **Accurate Predictions:**\n",
    "   - The combination of diverse decision trees in the ensemble allows the model to capture complex relationships in the data, leading to accurate predictions of land cover types.\n",
    "\n",
    "3. **Handling Noisy Data:**\n",
    "   - By training on different subsets of the data, the ensemble is less sensitive to noise and outliers present in individual images, making the model more resilient.\n",
    "\n",
    "4. **Improved Generalization:**\n",
    "   - Bagging enhances the generalization capability of the model, allowing it to perform well on new satellite images that were not part of the training dataset.\n",
    "\n",
    "This application of bagging in land cover classification demonstrates how the technique can address challenges in remote sensing tasks, providing accurate and robust models for monitoring and managing land cover over large geographical areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acf0e9-4ed1-4fbb-962f-803762b1ac7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86035f1e-5219-485c-8476-d7446c0bf598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ab3e8-49d3-438b-ac83-d943a7badf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc73cf8-236e-4d3c-b822-16c5f68cddff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cdcea-fa82-40b8-bcfb-dc05ad78d1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ed613-a549-4744-a198-62f54caf9e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916e0f9-486f-4b85-8e7d-e431cb5e9f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa375a3-1d3d-4fd2-9a2d-6fe4708f0256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b46773-2cab-4e8b-875b-575a56b2aa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4407a1-fae5-4e8a-a927-a891362b86b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa146a7-eff5-41b8-83f0-86c50c75dfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2ebf2-5329-4110-a35b-fc25d29febf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da5d77-53d7-4710-ba6d-6de735beb4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387502dd-75e9-4b37-8a67-1c58ffb5e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5dcfd-3f10-4014-a070-7e5de39a9f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124c0b5-166e-4e15-8990-411d1e94aada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c4abe-ef56-4eb7-8c37-e093263661a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8cc669-8147-472f-9fbe-7f09455169a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d8d55-5eec-4c89-aa8d-b5e42f99180e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14dac9-2fbb-4d6a-991a-c1da8bd7667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd95b9-e94b-4f53-8bd2-1c06da8efe02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1e9d6-ed6e-4f8f-bd6e-3a43553c2a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e945b89-c1d8-4872-89f8-9579dc1a696f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08523b80-65a1-4511-983a-886265d59e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476b421-5ed2-45bb-aa40-0e59f21cc401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d4d0d-9115-4538-8392-edab9d756508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e432a43-65e9-4763-9bd4-dcbabafb71f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cebfb-0129-44e2-9052-bb66f9d2e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5066f-4e62-4054-adcf-333582c8aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
